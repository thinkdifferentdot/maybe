---
phase: 08-validation-testing
plan: 02
type: execute
domain: rails-testing
---

<objective>
Verify all OpenAI functionality still works correctly (regression check) after Anthropic integration changes.

Purpose: Ensure that adding Anthropic support did not break existing OpenAI functionality. This is critical - we modified shared code (Provider::Registry, Setting model, LlmUsage) and must confirm OpenAI users are unaffected.

Output: Confirmed passing test suite for Provider::Openai with no regressions.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/08-validation-testing/08-CONTEXT.md

# Key files modified during Anthropic integration (may affect OpenAI):
@app/models/provider/registry.rb - Added anthropic method, updated available_providers
@app/models/setting.rb - Added anthropic_access_token, anthropic_model, llm_provider fields
@app/models/llm_usage.rb - Added Anthropic pricing

# OpenAI provider file (should be unchanged):
@app/models/provider/openai.rb

# Reference test file:
@test/models/provider/openai_test.rb

# Existing VCR cassettes (should still work):
@test/vcr_cassettes/openai/*.yml

**Tech stack available:**
- ruby-openai gem v8.1.0
- Rails test framework: Minitest with fixtures
- VCR for API call recording/replay

**Established patterns:**
- OpenAI tests already exist and should pass
- VCR cassettes already recorded
- Run existing tests to verify no regression

**Constraining decisions:**
- Phase 4-01: Used Setting bracket notation to avoid Phase 5 dependencies
- Phase 5-02: Updated to use Setting method calls (Phase 5 complete)
- Changes to Provider::Registry should not affect openai method behavior
- Changes to Setting model added new fields but shouldn't modify existing behavior

**Issues being addressed:** None
</context>

<tasks>

<task type="auto">
  <name>Task 1: Run existing OpenAI tests to verify no regression</name>
  <files>test/models/provider/openai_test.rb</files>
  <action>Run the existing OpenAI test suite to check for any regressions:
- Execute: bin/rails test test/models/provider/openai_test.rb
- All existing tests should pass without modification
- If tests fail, identify which code changes caused the regression
- DO NOT modify tests - fix any regressions in the implementation code

Potential regression sources to check if tests fail:
- Provider::Registry.openai method behavior changed
- Setting model changes affected openai_access_token or openai_model behavior
- LlmUsage changes affected cost calculation for OpenAI models</action>
  <verify>bin/rails test test/models/provider/openai_test.rb exits with status 0 (all tests pass)</verify>
  <done>All existing OpenAI tests passing with no code changes needed</done>
</task>

<task type="auto">
  <name>Task 2: Verify OpenAI registry integration still works</name>
  <files>test/models/provider/registry_test.rb</files>
  <action>Run existing registry tests that cover OpenAI:
- Execute: bin/rails test test/models/provider/registry_test.rb
- Pay special attention to tests involving:
  - openai provider configuration
  - ENV vs Setting fallback logic
  - available_providers for :llm concept
- These tests verify that adding :anthropic to available_providers didn't break :openai</action>
  <verify>bin/rails test test/models/provider/registry_test.rb passes all tests</verify>
  <done>Registry tests for OpenAI passing</done>
</task>

<task type="auto">
  <name>Task 3: Verify LlmUsage.calculate_cost for OpenAI models</name>
  <files>test/models/llm_usage_test.rb</files>
  <action>Create or run tests for LlmUsage.calculate_cost to verify OpenAI pricing still works:
- Test with gpt-4.1 model (standard OpenAI model)
- Test with gpt-4.1-mini (cheaper variant)
- Verify cost calculation returns correct USD values
- Verify infer_provider returns "openai" for OpenAI models

If test/models/llm_usage_test.rb doesn't exist, create it with these tests:
```ruby
test "calculates cost for OpenAI gpt-4.1 model" do
  cost = LlmUsage.calculate_cost(model: "gpt-4.1", prompt_tokens: 1000, completion_tokens: 1000)
  assert_in_delta 0.03, cost, 0.001  # Expected cost based on pricing
end

test "infers openai provider from model name" do
  assert_equal "openai", LlmUsage.infer_provider("gpt-4.1")
  assert_equal "openai", LlmUsage.infer_provider("gpt-4.1-mini")
end
```</action>
  <verify>bin/rails test test/models/llm_usage_test.rb -n /openai/ or /cost/ passes</verify>
  <done>OpenAI cost calculation tests passing</done>
</task>

<task type="auto">
  <name>Task 4: Verify Setting model changes don't affect OpenAI fields</name>
  <files>test/models/setting_test.rb</files>
  <action>Run Setting model tests to verify OpenAI fields still work:
- Execute: bin/rails test test/models/setting_test.rb
- Verify openai_access_token field still accessible
- Verify openai_model field still accessible
- Verify ENV fallback still works (OPENAI_ACCESS_TOKEN, OPENAI_MODEL)
- Verify update handlers for OpenAI fields still work

If Setting tests don't exist or don't cover OpenAI, add tests:
```ruby
test "openai_access_token field works" do
  setting = Setting.new
  setting.openai_access_token = "test-token"
  assert_equal "test-token", setting.openai_access_token
end

test "openai_access_token falls back to ENV" do
  ClimateControl.modify OPENAI_ACCESS_TOKEN: "env-token" do
    assert_equal "env-token", Setting.openai_access_token
  end
end
```</action>
  <verify>bin/rails test test/models/setting_test.rb passes</verify>
  <done>Setting tests for OpenAI fields passing</done>
</task>

<task type="auto">
  <name>Task 5: Run full test suite for any unexpected failures</name>
  <files></files>
  <action>Run the complete test suite to catch any unexpected regressions:
- Execute: bin/rails test
- Review any failures related to:
  - AI features (chat, categorization, merchant detection)
  - Provider initialization
  - Settings UI
  - LLM usage tracking
- Fix any regressions found (modify implementation, not tests)
- Document any failures that are unrelated to Anthropic integration

This is a broad safety net to catch issues we didn't anticipate.</action>
  <verify>bin/rails test completes with expected pass rate (account for any pre-existing failures)</verify>
  <done>Full test suite run completed, regressions identified and fixed or documented</done>
</task>

<task type="auto">
  <name>Task 6: Document regression test results</name>
  <files>.planning/phases/08-validation-testing/08-02-REGRESSION-REPORT.md</files>
  <action>Create a regression report documenting:
1. Test results summary:
   - OpenAI provider tests: X/Y passed
   - Registry tests: X/Y passed
   - LlmUsage tests: X/Y passed
   - Setting tests: X/Y passed
   - Full suite: X/Y passed
2. Any regressions found and fixed
3. Any pre-existing test failures (unrelated to Anthropic)
4. Conclusion: OpenAI functionality verified intact

Format:
```markdown
# OpenAI Regression Report

## Test Results

### Provider::Openai Tests
- Status: PASSED / FAILED
- Details: [summary]

### Registry Tests (OpenAI-related)
- Status: PASSED / FAILED
- Details: [summary]

### LlmUsage Tests (OpenAI pricing)
- Status: PASSED / FAILED
- Details: [summary]

### Setting Tests (OpenAI fields)
- Status: PASSED / FAILED
- Details: [summary]

## Regressions Found

[None or list of issues found and fixed]

## Pre-existing Issues

[None or list of unrelated failures]

## Conclusion

OpenAI functionality is [VERIFIED INTACT / HAS REGRESSIONS]
```</action>
  <verify>08-02-REGRESSION-REPORT.md exists with complete test results</verify>
  <done>Regression report created documenting OpenAI verification</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] bin/rails test test/models/provider/openai_test.rb passes
- [ ] bin/rails test test/models/provider/registry_test.rb passes
- [ ] bin/rails test test/models/setting_test.rb passes (or OpenAI-specific tests added)
- [ ] bin/rails test completes without new failures
- [ ] 08-02-REGRESSION-REPORT.md created with results
- [ ] Any regressions found have been fixed
</verification>

<success_criteria>

- All OpenAI provider tests passing
- Registry integration for OpenAI verified
- Setting model OpenAI fields verified
- LlmUsage cost calculation for OpenAI verified
- Full test suite run completed
- Regression report documents verification
  </success_criteria>

<output>
After completion, create `.planning/phases/08-validation-testing/08-02-SUMMARY.md`:

# Phase 8 Plan 2: OpenAI Regression Tests Summary

**Verified OpenAI functionality remains intact after Anthropic integration**

## Accomplishments

- Ran existing OpenAI test suite - all tests passing
- Verified registry integration for OpenAI unaffected
- Verified Setting model changes don't impact OpenAI fields
- Verified LlmUsage cost calculation for OpenAI models
- Created regression report documenting verification

## Files Created/Modified

- `.planning/phases/08-validation-testing/08-02-REGRESSION-REPORT.md` - Regression verification report

## Test Results

[Summary from regression report]

## Regressions Found

[None or list of issues found and fixed]

## Commits

- [List commits produced]

## Next Step

Ready for 08-03-PLAN.md (Provider switching and settings UI tests)
</output>