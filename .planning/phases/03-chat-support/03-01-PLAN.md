---
phase: 03-chat-support
plan: 01
type: execute
domain: rails-models
---

<objective>
Implement basic chat_response for Anthropic without tool calling support.

Purpose: Establish the foundation for Anthropic chat functionality using the Messages API, following the same architectural patterns as Provider::Openai but adapted for Anthropic's API format.

Output: Working chat_response method that handles simple text conversations (no tools), with proper Langfuse tracing and LlmUsage recording.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
~/.claude/get-shit-done/templates/phase-prompt.md
~/.claude/get-shit-done/references/plan-format.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/03-chat-support/03-CONTEXT.md
@.planning/phases/03-chat-support/03-RESEARCH.md
@.planning/phases/1-foundation/1-02-SUMMARY.md
@app/models/provider/anthropic.rb
@app/models/provider/openai.rb
@app/models/provider/openai/chat_config.rb
@app/models/provider/openai/chat_parser.rb
@app/models/provider/llm_concept.rb

**Tech stack available:** anthropic gem ~> 1.16.0 (from Phase 1)
**Established patterns:** Provider::Openai chat_response structure, ChatConfig/ChatParser pattern, Langfuse tracing, LlmUsage recording

**Constraining decisions:**
- Phase 1-02: DEFAULT_MODEL is "claude-sonnet-4-5-20250929", model prefix matching uses start_with?
- Phase 1-01: Use official anthropic gem (not community ruby-anthropic)

**Key decisions from RESEARCH.md:**
- Use manual tool handling (not beta.messages.tool_runner) for consistency with OpenAI provider
- Map token field names: input_tokens -> prompt_tokens, output_tokens -> completion_tokens
- Always include max_tokens parameter (required by Anthropic, unlike OpenAI)
- Model accepts both string and symbol, use string from settings
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Provider::Anthropic::ChatConfig for basic message building</name>
  <files>app/models/provider/anthropic/chat_config.rb</files>
  <action>Create ChatConfig class following Openai::ChatConfig pattern but adapted for Anthropic Messages API format:

- Initialize with functions: [] (empty for now, will add in 03-02) and function_results: []
- Define build_input(prompt) that returns messages array in Anthropic format:
  - {role: "user", content: prompt}
  - No function_call_output handling yet (03-03)
- Define tools method returning empty array for now (will implement in 03-02)

Key differences from OpenAI:
- Anthropic uses messages array, not "input" key with role/content items
- No "function_call_output" type (uses "tool_result" in 03-03)
- Simpler structure for basic text-only chat

No tools parameter handling yet - that's plan 03-02.</action>
  <verify>File exists, class loads with `rails runner "puts Provider::Anthropic::ChatConfig.new(build_input('test'))"</verify>
  <done>ChatConfig class creates correct Anthropic messages format for simple text prompts</done>
</task>

<task type="auto">
  <name>Task 2: Create Provider::Anthropic::ChatParser for response parsing</name>
  <files>app/models/provider/anthropic/chat_parser.rb</files>
  <action>Create ChatParser class following Openai::ChatParser pattern but adapted for Anthropic Messages API response format:

- Initialize with Anthropic response object
- Define parsed method returning Provider::LlmConcept::ChatResponse with:
  - id: response["id"] (Anthropic provides response id)
  - model: response["model"]
  - messages: Array of Provider::LlmConcept::ChatMessage from text blocks
  - function_requests: empty array for now (will add in 03-02)

Extract messages from response.content:
- Filter blocks where block.type == "text"
- Extract block.text for output_text
- Join multiple text blocks with newline if present

No tool_use handling yet - that's plan 03-02.</action>
  <verify>File exists, parses Anthropic response structure correctly</verify>
  <done>ChatParser converts Anthropic response to LlmConcept::ChatResponse format</done>
</task>

<task type="auto">
  <name>Task 3: Implement chat_response method in Provider::Anthropic</name>
  <files>app/models/provider/anthropic.rb</files>
  <action>Add chat_response method to Provider::Anthropic following Provider::Openai pattern but adapted for Anthropic Messages API:

Method signature (matches LlmConcept):
def chat_response(prompt, model:, instructions: nil, functions: [], function_results: [], streamer: nil, previous_response_id: nil, session_id: nil, user_identifier: nil, family: nil)

Implementation:
1. Wrap with with_provider_response (inherited from Provider)
2. Create ChatConfig with functions/function_results (empty for now)
3. Create Langfuse trace with name "anthropic.chat_response"
4. Build messages via chat_config.build_input(prompt)
5. Add system message if instructions present (Anthropic uses separate "system" parameter, not in messages array)
6. Call client.messages.create with:
   - model: model
   - max_tokens: 4096 (REQUIRED by Anthropic)
   - messages: messages array
   - system: instructions (if present)
7. Parse response with ChatParser
8. Log Langfuse generation with input/output/usage
   - Map Anthropic usage: input_tokens -> prompt_tokens, output_tokens -> completion_tokens
   - Note: Anthropic uses different field names than OpenAI
9. Record LlmUsage with mapped token counts
10. Return parsed ChatResponse

For this plan (03-01), functions and function_results are ignored (handled in 03-02/03-03).
Streamer is ignored (handled in 03-04).
Previous_response_id is ignored (Anthropic doesn't have this feature - manage conversation history manually via messages array).

Error handling:
- Rescue Anthropic::Errors::* exceptions
- Log to Langfuse with error
- Record failed LlmUsage
- Re-raise as Provider::Anthropic::Error</action>
  <verify>Method handles simple text conversations, records LlmUsage correctly, logs to Langfuse</verify>
  <done>chat_response works for basic text-only conversations with proper tracing</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] ChatConfig builds correct Anthropic messages format
- [ ] ChatParser correctly extracts text from Anthropic response
- [ ] chat_response handles simple text prompts
- [ ] Token usage is correctly mapped (input_tokens -> prompt_tokens, output_tokens -> completion_tokens)
- [ ] Langfuse traces are created and generations logged
- [ ] LlmUsage records are created with correct token counts
- [ ] max_tokens parameter is included in API call
- [ ] System instructions are passed correctly (system parameter, not in messages array)
</verification>

<success_criteria>

- All tasks completed
- Basic text-only chat works through Anthropic
- Token counting matches Anthropic API format
- Observability (Langfuse) tracks Anthropic requests
- No errors or warnings introduced
  </success_criteria>

<output>
After completion, create `.planning/phases/03-chat-support/03-01-SUMMARY.md`:

# Phase 3 Plan 1: Basic Chat Support Summary

**Implemented basic chat_response for Anthropic using Messages API with proper Langfuse tracing and LlmUsage recording**

## Accomplishments

- Created Provider::Anthropic::ChatConfig for Anthropic message format
- Created Provider::Anthropic::ChatParser for parsing Anthropic responses
- Implemented chat_response method with system instructions support
- Mapped Anthropic token fields (input/output -> prompt/completion)
- Added Langfuse tracing for Anthropic chat requests

## Files Created/Modified

- `app/models/provider/anthropic/chat_config.rb` - Anthropic message builder
- `app/models/provider/anthropic/chat_parser.rb` - Anthropic response parser
- `app/models/provider/anthropic.rb` - Added chat_response method

## Decisions Made

- Used separate "system" parameter for instructions (Anthropic convention, not in messages array)
- Set max_tokens to 4096 as default (required by Anthropic API)
- Mapped token field names: input_tokens -> prompt_tokens, output_tokens -> completion_tokens

## Issues Encountered

[None expected, document any issues that arise]

## Next Step

Ready for 03-02-PLAN.md - Add tool/function calling support
</output>
