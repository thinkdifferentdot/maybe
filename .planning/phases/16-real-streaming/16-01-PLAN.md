---
phase: 16-real-streaming
type: execute
domain: rails-backend
---

<objective>
Implement true streaming for Anthropic chat responses using the anthropic Ruby SDK's MessageStream API, matching OpenAI's streaming behavior.

Purpose: Enable token-by-token streaming for Anthropic chat responses, providing the same real-time user experience as OpenAI provider.

Output: Working streaming chat responses via Provider::Anthropic that emit ChatStreamChunk events to Assistant::Responder.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/16-real-streaming/16-RESEARCH.md
@.planning/phases/16-real-streaming/16-CONTEXT.md
@app/models/provider/llm_concept.rb
@app/models/provider/anthropic.rb
@app/models/provider/anthropic/chat_config.rb
@app/models/provider/anthropic/chat_parser.rb
@app/models/provider/openai/chat_stream_parser.rb
@app/models/provider/openai.rb
@app/models/assistant/responder.rb

**Tech stack available:**
- anthropic gem ~> 1.16.0 (already installed, provides MessageStream API)
- Ruby 3.2.0+

**Established patterns:**
- OpenAI streaming: ChatStreamParser converts raw events to ChatStreamChunk
- Streamer proc pattern: passed to chat_response, called with ChatStreamChunk(type:, data:, usage:)
- Assistant::Responder handles "output_text" and "response" chunk types
- Usage tracking via record_llm_usage with Langfuse integration

**Constraining decisions:**
- Phase 03-04: Streaming deferred intentionally - follow OpenAI pattern
- Use anthropic.messages.stream() with stream.each for full event access (not stream.text.each - tool use requires full events)
- Chat only - auto_categorize and auto_detect_merchants don't need streaming (async)

**Issues being addressed:** None

**Research context (from 16-RESEARCH.md):**
- Use client.messages.stream(parameters) which returns MessageStream
- Event types: content_block_start, content_block_delta, content_block_stop, message_delta, message_stop, ping, error
- content_block_delta with delta.type=:text_delta yields text chunks
- message_delta contains usage (input_tokens, output_tokens)
- stream.__accumulated_message__ provides complete Message after stream consumption
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Provider::Anthropic::ChatStreamParser class</name>
  <files>app/models/provider/anthropic/chat_stream_parser.rb</files>
  <action>Create new ChatStreamParser class following the OpenAI pattern. The parser converts Anthropic MessageStream events to ChatStreamChunk format.

Key implementation points:
- Initialize with an event from Anthropic's MessageStream (event is a Struct with #type method)
- Handle :content_block_delta events with delta.type == :text_delta → Chunk(type: "output_text", data: event.delta.text, usage: nil)
- Handle :message_delta events → accumulate usage metadata (input_tokens, output_tokens)
- Handle :message_stop events → Chunk(type: "response", data: accumulated_response, usage: accumulated_usage)
- Track tool use accumulation for :content_block_start/:content_block_delta/:content_block_stop sequence
- Return nil for unhandled event types (ping, unknown types)

Follow the OpenAI ChatStreamParser pattern:
- Error = Class.new(StandardError)
- Use ChatStreamChunk = Provider::LlmConcept::ChatStreamChunk
- Use ChatResponse = Provider::LlmConcept::ChatResponse
- Use ChatMessage = Provider::LlmConcept::ChatMessage

IMPORTANT: Use event.type (symbol), NOT event["type"] - Anthropic SDK returns Structs, not Hashes.
</action>
  <verify>File exists, class defined with Error constant, parsed method returns Chunk or nil for handled/unhandled events</verify>
  <done>ChatStreamParser class created with event type handling for text deltas and message completion</done>
</task>

<task type="auto">
  <name>Task 2: Update Provider::Anthropic#chat_response to use streaming</name>
  <files>app/models/provider/anthropic.rb</files>
  <action>Modify chat_response method to use client.messages.stream() when streamer is provided.

Implementation changes:
1. When streamer.present?, use client.messages.stream(parameters) instead of client.messages.create(parameters)
2. Create collected_chunks array to track response chunks (follows OpenAI pattern)
3. Create stream_proxy proc that:
   - Iterates stream.each { |event| ... }
   - Parses each event with ChatStreamParser.new(event).parsed
   - Calls streamer.call(parsed_chunk) if parsed_chunk is not nil
   - Appends parsed_chunk to collected_chunks
4. After stream completes, extract response and usage from collected_chunks
5. Log Langfuse generation and record usage with extracted usage
6. Return response.data from the "response" type chunk

Key differences from OpenAI:
- Anthropic uses stream.each (yields event Structs), not OpenAI's hash-based chunks
- Event.type is a symbol (:content_block_delta), not a string
- Usage comes from :message_delta event, not embedded in response

Remove the TODO comment and manual streamer simulation code (lines 109-172 in current implementation).

IMPORTANT: client.messages.stream() returns a MessageStream that must be consumed via .each. Accessing __accumulated_message__ before consumption will block.
</action>
  <verify>grep -n "TODO.*streaming" app/models/provider/anthropic.rb returns no results; grep -n "messages.stream" app/models/provider/anthropic.rb returns match</verify>
  <done>chat_response uses MessageStream when streamer provided; emits ChatStreamChunk events; returns parsed response</done>
</task>

<task type="auto">
  <name>Task 3: Add streaming spec for ChatStreamParser</name>
  <files>test/models/provider/anthropic/chat_stream_parser_test.rb</files>
  <action>Create test file for ChatStreamParser following the OpenAI testing pattern.

Test cases:
1. Text delta event → returns output_text chunk with delta text
2. Message delta event → accumulates usage (does not return chunk)
3. Message stop event → returns response chunk with accumulated data
4. Unknown event type (ping) → returns nil
5. Tool use content_block_start → tracked internally (no chunk returned)
6. Tool use content_block_delta with input_json_delta → accumulated (no chunk returned)

Use OpenStruct or mock objects to simulate Anthropic event Structs:
- event = OpenStruct.new(type: :content_block_delta, delta: OpenStruct.new(type: :text_delta, text: "Hello"))
- event = OpenStruct.new(type: :message_delta, usage: OpenStruct.new(input_tokens: 10, output_tokens: 20))
- event = OpenStruct.new(type: :message_stop)

Follow Minitest conventions (not RSpec), use fixtures where applicable.
</action>
  <verify>bin/rails test test/models/provider/anthropic/chat_stream_parser_test.rb passes all tests</verify>
  <done>ChatStreamParser tests covering text deltas, usage accumulation, message stop, and tool use events</done>
</task>

</tasks>

<verification>
Before declaring phase complete:
- [ ] `bin/rails test test/models/provider/anthropic/chat_stream_parser_test.rb` passes
- [ ] `bin/rails test test/models/assistant_test.rb` passes (regression check for Assistant::Responder)
- [ ] Manual test: Chat with Anthropic provider shows progressive text output (not all-at-once)
- [ ] No errors in Rails logs during streaming
- [ ] Usage recorded correctly (check LlmUsage records after chat)
</verification>

<success_criteria>

- ChatStreamParser converts Anthropic events to ChatStreamChunks correctly
- Anthropic chat_response streams token-by-token when streamer provided
- OpenAI streaming still works (regression test)
- Usage metadata tracked correctly for streaming responses
  </success_criteria>

<output>
After completion, create `.planning/phases/16-real-streaming/16-01-SUMMARY.md`:

# Phase 16 Plan 1: Real Streaming Support Summary

**Implemented token-by-token streaming for Anthropic chat responses using MessageStream API**

## Accomplishments

- Created Provider::Anthropic::ChatStreamParser to convert MessageStream events to ChatStreamChunk format
- Updated Provider::Anthropic#chat_response to use client.messages.stream() when streamer provided
- Added comprehensive test coverage for ChatStreamParser event handling
- Removed deferred streaming TODO, implemented actual streaming

## Files Created/Modified

- `app/models/provider/anthropic/chat_stream_parser.rb` - NEW: Parses Anthropic stream events
- `app/models/provider/anthropic.rb` - MODIFIED: Added streaming support in chat_response
- `test/models/provider/anthropic/chat_stream_parser_test.rb` - NEW: Parser tests

## Decisions Made

- Use stream.each (full event access) instead of stream.text.each to support future tool use streaming
- Return nil for unhandled event types (ping, unknown) - graceful degradation
- Follow OpenAI's collected_chunks pattern to extract response/usage after stream completes

## Issues Encountered

[Document any issues or "None"]

## Next Phase Readiness

Phase 16 complete. Ready for Phase 17: Auto-Categorization Test Coverage.
</output>
